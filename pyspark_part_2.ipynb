{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('nulls_handling.csv',header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|    John|  25|         7|\n",
      "|   Emily|  30|      NULL|\n",
      "| Michael|  28|         9|\n",
      "|   Sarah|NULL|      NULL|\n",
      "|   David|  40|         8|\n",
      "| Jessica|NULL|         5|\n",
      "|  Daniel|  33|      NULL|\n",
      "|  Rachel|  29|         6|\n",
      "| Matthew|NULL|         4|\n",
      "|Jennifer|  31|        10|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "| Age|Experience|\n",
      "+----+----------+\n",
      "|  25|         7|\n",
      "|  30|      NULL|\n",
      "|  28|         9|\n",
      "|NULL|         2|\n",
      "|  40|         8|\n",
      "|NULL|         5|\n",
      "|  33|      NULL|\n",
      "|  29|         6|\n",
      "|NULL|         4|\n",
      "|  31|        10|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop columns\n",
    "df_spark.drop('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    John| 25|         7|\n",
      "| Michael| 28|         9|\n",
      "|   David| 40|         8|\n",
      "|  Rachel| 29|         6|\n",
      "|Jennifer| 31|        10|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop all nulls\n",
    "df_spark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|    John|  25|         7|\n",
      "|   Emily|  30|      NULL|\n",
      "| Michael|  28|         9|\n",
      "|   Sarah|NULL|         2|\n",
      "|   David|  40|         8|\n",
      "| Jessica|NULL|         5|\n",
      "|  Daniel|  33|      NULL|\n",
      "|  Rachel|  29|         6|\n",
      "| Matthew|NULL|         4|\n",
      "|Jennifer|  31|        10|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## all==how, drop all null rows\n",
    "df_spark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|    John|  25|         7|\n",
      "|   Emily|  30|      NULL|\n",
      "| Michael|  28|         9|\n",
      "|   David|  40|         8|\n",
      "| Jessica|NULL|         5|\n",
      "|  Daniel|  33|      NULL|\n",
      "|  Rachel|  29|         6|\n",
      "| Matthew|NULL|         4|\n",
      "|Jennifer|  31|        10|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#threshold\n",
    "df_spark.na.drop(how=\"all\",thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    John| 25|         7|\n",
      "|   Emily| 30|      NULL|\n",
      "| Michael| 28|         9|\n",
      "|   David| 40|         8|\n",
      "|  Daniel| 33|      NULL|\n",
      "|  Rachel| 29|         6|\n",
      "|Jennifer| 31|        10|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Subset\n",
    "df_spark.na.drop(how=\"all\",subset=['Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    John| 25|         7|\n",
      "|   Emily| 30|         1|\n",
      "| Michael| 28|         9|\n",
      "|   Sarah|  1|         1|\n",
      "|   David| 40|         8|\n",
      "| Jessica|  1|         5|\n",
      "|  Daniel| 33|         1|\n",
      "|  Rachel| 29|         6|\n",
      "| Matthew|  1|         4|\n",
      "|Jennifer| 31|        10|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filling the missing values\n",
    "df_spark.na.fill(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|    John|  25|         7|\n",
      "|   Emily|  30|         1|\n",
      "| Michael|  28|         9|\n",
      "|   Sarah|NULL|         1|\n",
      "|   David|  40|         8|\n",
      "| Jessica|NULL|         5|\n",
      "|  Daniel|  33|         1|\n",
      "|  Rachel|  29|         6|\n",
      "| Matthew|NULL|         4|\n",
      "|Jennifer|  31|        10|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filling the missing values\n",
    "df_spark.na.fill(1,['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|    John|  25|         7|\n",
      "|   Emily|  30|      NULL|\n",
      "| Michael|  28|         9|\n",
      "|   Sarah|NULL|      NULL|\n",
      "|   David|  40|         8|\n",
      "| Jessica|NULL|         5|\n",
      "|  Daniel|  33|      NULL|\n",
      "|  Rachel|  29|         6|\n",
      "| Matthew|NULL|         4|\n",
      "|Jennifer|  31|        10|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace nulls with mean \n",
    "from pyspark.ml.feature import Imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input columns and output columns separately for each imputation\n",
    "input_cols = ['Age', 'Experience', 'Salary']\n",
    "output_cols = [\"{}_imputed\".format(c) for c in input_cols]\n",
    "\n",
    "# Create separate Imputer instances for each column\n",
    "imputers = [\n",
    "    Imputer(inputCol=col, outputCol=output_col).setStrategy(\"mean\")\n",
    "    for col, output_col in zip(input_cols, output_cols)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "pipeline = Pipeline(stages=imputers)\n",
    "model = pipeline.fit(df_spark)\n",
    "df_imputed = model.transform(df_spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    John|  25|         7| 20000|         25|                 7|         20000|\n",
      "|   Emily|  30|      NULL| 15000|         30|                 7|         15000|\n",
      "| Michael|  28|         9|  8000|         28|                 9|          8000|\n",
      "|   Sarah|NULL|      NULL| 14000|         30|                 7|         14000|\n",
      "|   David|  40|         8| 25000|         40|                 8|         25000|\n",
      "| Jessica|NULL|         5|  NULL|         30|                 5|         14625|\n",
      "|  Daniel|  33|      NULL|  5000|         33|                 7|          5000|\n",
      "|  Rachel|  29|         6| 18000|         29|                 6|         18000|\n",
      "| Matthew|NULL|         4|  NULL|         30|                 4|         14625|\n",
      "|Jennifer|  31|        10| 12000|         31|                10|         12000|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imputed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    John|  25|         7| 20000|         25|                 7|         20000|\n",
      "|   Emily|  30|      NULL| 15000|         30|                 7|         15000|\n",
      "| Michael|  28|         9|  8000|         28|                 9|          8000|\n",
      "|   Sarah|NULL|      NULL| 14000|         30|                 7|         14000|\n",
      "|   David|  40|         8| 25000|         40|                 8|         25000|\n",
      "| Jessica|NULL|         5|  NULL|         30|                 5|         14000|\n",
      "|  Daniel|  33|      NULL|  5000|         33|                 7|          5000|\n",
      "|  Rachel|  29|         6| 18000|         29|                 6|         18000|\n",
      "| Matthew|NULL|         4|  NULL|         30|                 4|         14000|\n",
      "|Jennifer|  31|        10| 12000|         31|                10|         12000|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#median\n",
    "# Specify input columns and output columns separately for each imputation\n",
    "input_cols = ['Age', 'Experience', 'Salary']\n",
    "output_cols = [\"{}_imputed\".format(c) for c in input_cols]\n",
    "\n",
    "# Create separate Imputer instances for each column\n",
    "imputers = [\n",
    "    Imputer(inputCol=col, outputCol=output_col).setStrategy(\"median\")\n",
    "    for col, output_col in zip(input_cols, output_cols)\n",
    "]\n",
    "pipeline = Pipeline(stages=imputers)\n",
    "model = pipeline.fit(df_spark)\n",
    "df_imputed = model.transform(df_spark)\n",
    "df_imputed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
